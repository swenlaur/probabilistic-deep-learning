{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47679e3b",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Variational inference\n",
    "\n",
    "\\begin{align*}\n",
    "\\DeclareMathOperator*{\\argmax}{argmax}\n",
    "\\DeclareMathOperator*{\\EXPSYM}{\\mathbf{E}}\n",
    "\\newcommand{\\KL}[1]{\\mathbf{KL}\\left[#1\\right]}\n",
    "\\newcommand{\\EXP}[2]{\\EXPSYM_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\Kappa}{\\boldsymbol{\\kappa}}\n",
    "\\newcommand{\\Lambda}{\\boldsymbol{\\lambda}}\n",
    "\\newcommand{\\Theta}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\Data}{D}\n",
    "\\newcommand{\\FFF}{\\mathcal{F}}\n",
    "\\newcommand{\\LLL}{\\mathcal{L}}\n",
    "\\newcommand{\\NNN}{\\mathcal{N}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\appropto}{\\stackrel{\\propto}{\\sim}}\n",
    "\\end{align*}\n",
    "\n",
    "* [Kenneth Tay. Laplaceâ€™s approximation for Bayesian posterior distribution](https://statisticaloddsandends.wordpress.com/2019/07/11/laplaces-approximation-for-bayesian-posterior-distribution/)\n",
    "* [Charles Blundell et al. Weight Uncertainty in Neural Networks](https://arxiv.org/abs/1505.05424)\n",
    "* [Yeming Wen et al. Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches](https://arxiv.org/abs/1803.04386)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f15d99",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Variational inference</h1> </center>\n",
    "<center><h2>Theoretical overview</h2></center>\n",
    "<br>\n",
    "<center><h3>Sven Laur</h3></center>\n",
    "<center><h3>swen@ut.ee</h3></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c82bf61",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian neural networks\n",
    "\n",
    "<center>\n",
    "    <img src='./illustrations/bayesian_neural_network.png' width=20% alt='Necessity of reparametrisation'>\n",
    "</center>\n",
    "\n",
    "* Activation functions are kept deterministic.\n",
    "* Randomness is introduced into the weights of the neural network.\n",
    "* We can add randomness through pertubations. This is technically better.\n",
    "\n",
    "\\begin{align*}\n",
    "w_i\\sim\\NNN(\\mu,\\sigma)\\qquad\\Longleftrightarrow\\qquad w_i=\\mu+\\sigma\\cdot\\epsilon, \\quad \\epsilon\\sim\\NNN(0,1)\n",
    "\\end{align*}\n",
    "\n",
    "* This setup defines output probability even if all activation function are deterministic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dbb007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cd0e6a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Priors and regularisation\n",
    "\n",
    "When we search parameters with the highest posterior probability (MAP) then there are two commonly used estimates.\n",
    "\n",
    "* Maximum likelihood estimate for non-informative prior\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\vec{w}}_{MLE}=\\argmax_\\vec{w} \\log p[\\Data|\\vec{w}]\n",
    "\\end{align*}\n",
    "\n",
    "* Regularised cost function for more restrictive priors\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\vec{w}}_{MAP}=\\argmax_\\vec{w}\\log(\\vec{w}) + \\log p[\\Data|\\vec{w}]\n",
    "\\end{align*}\n",
    "\n",
    "Sometimes maximum likelihood and maximum aposteriori estimates are brittle. \n",
    "* The is a large region where the posterior is roughly the same for parameters.\n",
    "* We can overcome this problem by averaging predictions over different pararameter values.\n",
    "* To do that we must be able to sample from the posterior distribution.\n",
    "* Unfortunately, posterior distribution is unnormalised and thus direct sampling is hard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c238f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Laplace approximation\n",
    "\n",
    "Assume that the posterior is quite close to the normal distribution $\\NNN(\\vec{\\mu}, \\vec{\\Sigma})$:\n",
    "* There is a single mode and the propbability mass is concentrated around it.\n",
    "* This occurs naturally when the number of samples is large enough -- the law of large numbers.\n",
    "\n",
    "* Then we can find out the center $\\vec{\\mu}$ by computing the maximum aposteriori estimate $\\hat{\\vec{w}}_{MAP}$.\n",
    "* We can find the variance estimate by computing the Hessian (second derivative) $H$ from the unnormalised log-posterior \n",
    "\n",
    "\\begin{align*}\n",
    "h(\\vec{w})=\\log p(\\vec{w}) + \\log p[\\Data|\\vec{w}]\n",
    "\\end{align*}\n",
    "* As a result the second order Taylor approximation of unnormalised log-likelihood is\n",
    "\n",
    "\\begin{align*}\n",
    " h(\\hat{\\vec{w}}_{MAP}+\\Delta \\vec{w}) \\approx h(\\hat{\\vec{w}}_{MAP}) + \\frac{1}{2} \\Delta \\vec{w}^T H(\\hat{\\vec{w}}_{MAP})\\Delta \\vec{w} \\end{align*}\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b219393c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Laplace approximation\n",
    "\n",
    "Assume that the posterior is quite close to the normal distribution $\\NNN(\\vec{\\mu}, \\vec{\\Sigma})$:\n",
    "* There is a single mode and the propbability mass is concentrated around it.\n",
    "* This occurs naturally when the number of samples is large enough -- the law of large numbers.\n",
    "\n",
    "* Then we can find out the center $\\vec{\\mu}$ by computing the maximum aposteriori estimate $\\hat{\\vec{w}}_{MAP}$.\n",
    "\n",
    "* The second order Taylor approximation of unnormalised log-likelihood is\n",
    "\n",
    "\\begin{align*}\n",
    " h(\\hat{\\vec{w}}_{MAP}+\\Delta \\vec{w}) \\approx h(\\hat{\\vec{w}}_{MAP}) + \\frac{1}{2} \\Delta \\vec{w}^T H(\\hat{\\vec{w}}_{MAP})\\Delta \\vec{w} \\end{align*}\n",
    "\n",
    "* Thus the unnormalised posterior can be approximated\n",
    "\n",
    "\\begin{align*}\n",
    " p(\\vec{w}|\\Data) \\propto  exp\\left( \\frac{1}{2} \\Delta (\\vec{w}-\\hat{\\vec{w}}_{MAP}) H(\\vec{w}-\\hat{\\vec{w}}_{MAP})\\right)\\cdot \\mathcal{O}(1+ ||\\Delta \\vec{w}||) \n",
    "\\end{align*}\n",
    "\n",
    "* The approximation reveals the parameters of the normal distribution and we can sample the weights of the neural nwetwork.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32679e24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variational inference\n",
    "\n",
    "<center>\n",
    "    <img src='./illustrations/variational_inference.png' width=20% alt='Necessity of reparametrisation'>\n",
    "</center>\n",
    "\n",
    "* Laplace approximation does not work well for multimodal distributions.\n",
    "* We need to find a global distance measure for approximating the posterior distribution.\n",
    "* We still need a family of parametrised distributions $q_\\Lambda$ to approximate posterior.\n",
    "* Kullback-Leibler divergence is standard tool for that but we need to careful to not to get into trouble with unnormalised posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638f0670",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variational inference\n",
    "\n",
    "Let us define the cost function (**variational free energy**) \n",
    "\n",
    "\\begin{align*}\n",
    "\\FFF(\\Lambda)=\\KL{q_\\Lambda(\\Theta) || p[\\Theta|\\Data] }\n",
    "\\end{align*}\n",
    "\n",
    "where the network parameters are denoted by  $\\Theta$ instead of $\\vec{w}$ for unknown reasons.\n",
    "\n",
    "As simple manipulation yields\n",
    "\n",
    "\\begin{align*}\n",
    "\\KL{q_\\Lambda(\\Theta) || p[\\Theta|\\Data] }&=\\EXP{\\Theta\\sim q_\\Lambda}{\\log \\frac{q_\\lambda(\\Theta)}{p[\\Theta|\\Data]}}=\n",
    "\\EXP{\\Theta\\sim q_\\Lambda}{\\log q_\\lambda(\\Theta)-\\log p[\\Theta] - \\log p[\\Data|\\Theta] +\\log P[\\Data]}\\\\\n",
    "&=\\log P[\\Data] + \\EXP{\\Theta\\sim q_\\Lambda}{\\log \\frac{q_\\lambda(\\Theta)}{p[\\Theta]}}-\\EXP{\\Theta\\sim q_\\Lambda}{\\log p[\\Data|\\Theta] +\\log P[\\Data]}\\\\\n",
    "&=\\log P[\\Data] + \\KL{q_\\Lambda(\\Theta) || p[\\Theta]}  -\\EXP{\\Theta\\sim q_\\Lambda}{\\log p[\\Data|\\Theta] +\\log P[\\Data]}\n",
    "\\end{align*}\n",
    "\n",
    "we can simplify the cost function\n",
    "\n",
    "\\begin{align*}\n",
    "\\FFF(\\Lambda)= \\KL{q_\\Lambda(\\Theta)||p(\\Theta)} - \\EXP{\\Theta\\sim q_\\Lambda}{\\log p[\\Data|\\Theta]}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfffff0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Initial analysis of the final minimisation goal\n",
    "\n",
    "Our goal is to find the optimal set of parameters $\\boldsymbol{\\lambda}$ that minimises variational free energy\n",
    "\n",
    "\\begin{align*}\n",
    "\\FFF(\\Lambda)= \\KL{q_\\Lambda(\\Theta)||p_\\Kappa(\\Theta)} + \\EXP{\\Theta\\sim q_\\Lambda}{\\log p[\\Data|\\Theta]}\n",
    "\\end{align*}\n",
    "\n",
    "The first term $\\KL{q_\\Lambda(\\Theta)||p_\\Kappa(\\Theta)}$ in the cost function is easier to handle:\n",
    "* We control the parametrisation of the prior $p_\\Kappa(\\Theta)$. \n",
    "* We control the parametrisation of the variational approximation $q_\\Lambda(\\Theta)$.\n",
    "* We can choose parametrisations so that $\\KL{q_\\Lambda(\\Theta)||p_\\Kappa(\\Theta)}$ can be found analytically or approximated as $g(\\Kappa,\\Lambda)$.\n",
    "\n",
    "The second term is determined by the structure of neural network:\n",
    "* It is highly non-linear and we have not control over it.\n",
    "* We cannot evaluate it analytically and we need to rely on Monte-Carlo integration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5923cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Naive hill-climbing algorithm \n",
    "\n",
    "To minimise $\\FFF(\\Lambda)$ we can try several different values $\\Lambda_1,\\ldots, \\Lambda_M$ around current estimate $\\Lambda$:\n",
    "\n",
    "* We can evaluate $\\FFF(\\Lambda_1), \\ldots, \\FFF(\\Lambda_M)$ and choose the lowest value as the next step. \n",
    "* We can approximate $\\FFF(\\Lambda_i)$ with a Monte-Carlo integration \n",
    "\n",
    "\\begin{align*}\n",
    "\\FFF(\\Lambda_i)\\approx \\KL{q_{\\Lambda_i}(\\Theta)||p_\\Kappa(\\Theta)} \n",
    "+ \\frac{1}{K}\\cdot \\sum_{j=1}^K \\log p[\\Data|\\Theta_j],\\qquad \\Theta_1, \\ldots, \\Theta_K\\sim q_{\\Lambda_i}  \n",
    "\\end{align*}\n",
    "\n",
    "* We can compute a linear or quadratic approximation $\\hat{\\FFF}(\\Lambda)$ for the cost function around $\\Lambda$ and find $\\Lambda_*$ that minimises $\\hat{\\FFF}(\\Lambda)$.\n",
    "\n",
    "\n",
    "This is terribly ineffective for several reasons:\n",
    "\n",
    "* We need to sample $M\\times K$ points from distributions $q_{\\Lambda_1},\\ldots, q_{\\Lambda_M}$\n",
    "* The resulting quadratic approximation is a second order method while sampling usually gives the first order quarantees.\n",
    "* Monte-Carlo integral is too imprecise in small neighbourhoods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b2af7f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stohhastic gradient decent as a way out\n",
    "\n",
    "The standard neural network minimisation algorithm can be viewed as follows:\n",
    "\n",
    "* We need to minimise \n",
    "\\begin{align*}\n",
    "\\EXP{\\vec{(x,y)}\\sim\\Data}{\\LLL_{\\vec{w}}(\\vec{x},y)}=\\frac{1}{N}\\cdot\\sum_{j=1}^N \\LLL_{\\vec{w}}(\\vec{x}_j, y_j)\n",
    "\\end{align*}\n",
    "\n",
    "* In the gradient decent algorithm $\\vec{w}_{i+1}=\\vec{w}_i-\\eta\\cdot \\Delta \\vec{w}_i$ we must compute\n",
    "\n",
    "\\begin{align*}\n",
    "\\Delta \\vec{w}_i= \\frac{\\partial}{\\partial\\vec{w}} \\left[\n",
    "\\EXP{\\vec{(x,y)}\\sim\\Data}{\\LLL_{\\vec{w}}(\\vec{x},y)}\\right]\\!\\biggl|_{\\,\\vec{w}=\\vec{w}_i}\n",
    "\\end{align*}\n",
    "\n",
    "* However, we can do a stohhastic updates $\\vec{w}_{i+1}=\\vec{w}_i-\\eta\\cdot \\widehat{\\Delta \\vec{w}}_i(\\omega)$ instead provided that \n",
    "\n",
    "\\begin{align*}\n",
    " \\EXP{\\omega}{\\widehat{\\Delta\\vec{w}}_i(\\omega)}=\\Delta \\vec{w}_i\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6ba140",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unbiased estimate for the gradient of variational free energy \n",
    "\n",
    "As the cost function is indeed an expected value\n",
    "\n",
    "\\begin{align*}\n",
    "\\FFF(\\Lambda)= \\EXP{\\Theta\\sim q_\\Lambda}{\\log \\frac{q_\\Lambda(\\Theta)}{p_\\Kappa(\\Theta)}- \\log p[\\Data|\\Theta]}\n",
    "\\end{align*}\n",
    "\n",
    "we need a way to push partial derivative under the expectation to get \n",
    "\n",
    "\\begin{align*}\n",
    "\\widehat{\\Delta \\vec{w}}(\\Theta)= \\frac{\\partial}{\\partial\\Lambda}\\left[ \n",
    "\\log q_\\Lambda(\\Theta) -\\log p_\\Kappa(\\Theta)- \\log p[\\Data|\\Theta] \\right], \\qquad \\Theta \\sim q_\\Lambda \n",
    "\\end{align*}\n",
    "\n",
    "Pushing the derivative under the expectation works only if we can reparametrise the distribution\n",
    "\n",
    "\\begin{align*}\n",
    "\\Theta= f(\\lambda, \\epsilon)\\qquad \\epsilon\\sim q_*\n",
    "\\end{align*}\n",
    "\n",
    "where the distribution $q_*$ does not depend on the parameters $\\Lambda$. \n",
    "\n",
    "**Important observation:** Normal distribution for the weights of neural network $\\Theta \\sim \\NNN(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$ fits the bill.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ff25c80",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why reparametrisation is necessary\n",
    "\n",
    "<center>\n",
    "<img src='./illustrations/reparametrisation.png' width=35% alt='Necessity of reparametrisation'>\n",
    "</center>\n",
    "\n",
    "Without reparametrisation it is technically impossible to take a derivative\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial Loss}{\\partial \\mu_a}=\\frac{\\partial Loss}{\\partial a}\\cdot\\color{red}{\\frac{\\partial a}{\\partial \\mu_a}} \n",
    "\\end{align*}\n",
    "\n",
    "* We can sample $a$ from $\\NNN(\\mu_a, \\mu_b)$ but then its a number and there is no dependence on $\\mu_a$.\n",
    "* With reparametrisation $a=\\mu_a+\\sigma_a\\cdot \\epsilon$ the dependence between $\\mu_a$ and $a$ remains even if $\\epsilon$ is sampled.\n",
    "* Obviously, the reparametrisation must creates the same distribution for the $a$ values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977215b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayes by Backprop algorithm [LeCun 1985, ..., Blundell 2015]\n",
    "\n",
    "\n",
    "* The resulting algorithm is still overly complex as $\\log p[\\Data|\\Theta]$ sums over the entrire data set.\n",
    "* We can avoid this by splitting the data into minibatches $\\Data_1,\\ldots, \\Data_M$\n",
    "\n",
    "* To get the analoque of stohhastic gradient decent we need to split the cost function \n",
    "\n",
    "\\begin{align*}\n",
    "\\FFF(\\Lambda)=\\FFF_1(\\lambda, \\Data_1)+\\cdots+ \\FFF(\\lambda,\\Data_M)\n",
    "\\end{align*}\n",
    "\n",
    "* We need to split the regularisation term $\\KL{q_{\\Lambda_i}(\\Theta)||p_\\Kappa(\\Theta)}$ between $M$ subfunctions.\n",
    "\n",
    "\n",
    "* Geometrically decreasing weights $\\pi_j$ in front of the regularisation term are good in practice: \n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_j=\\frac{2^{M-j}}{2^M-1}\\propto \\frac{1}{2^j}\n",
    "\\end{align*}\n",
    "\n",
    "* The first few minibatches force the distribution close to the prior while the remaining batches fir parameters according to the data.\n",
    "  \n",
    "* There seem to be a little difference whether the regularisation term $\\KL{q_{\\Lambda_i}(\\Theta)||p_\\Kappa(\\Theta)}$ is analytically expressed or sampled.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795bdece",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Flipout optimisation\n",
    "\n",
    "**Problem:** \n",
    "* A minibatch split -- approximation for $\\FFF_1(\\lambda, \\Data_j)$ -- shares weights between different samples $(\\vec{x_j}, y_j)$\n",
    "* Thus the stohhastic gradient $\\widehat{\\Delta w}$ for the minibatch sum does not contain independent terms. \n",
    "* These correlations increase the variance of $\\widehat{\\Delta w}$ and thus slow the convergence of Bayes by Backprop.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "* Manipulate weigths so that weight distribution is preserved but individual terms in the sum are decorrelated.\n",
    "* Assume that perturbation to individual weights are independent and pertubation distribution is symmetric.\n",
    "* By flipping the signs of individual weight perturbations we preserve the perturbation distribution.\n",
    "* The correlation between the same weight instances for different datapoints is zero.\n",
    "* As a result the minibatch sum has much smaller variance and the algorithm converges faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f99bd8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementation in TensorFlow Probability\n",
    "\n",
    "There are special layers with reparametrisation:\n",
    "\n",
    "* `tfp.layers.DenseReparametrisation`:  by default is uses Gaussian variational posterior\n",
    "* `tfp.layers.DenseFlipout`: the same as previous but with lower variance for the stohhastic gradient \n",
    "* `tfp.layers.ConvolutionXDReparameterization`: by default uses Gaussian variational posterior for convolution kernel\n",
    "* `tfp.layers.ConvolutionXDFlipout`: the same as previous but with lower variance for the stohhastic gradient \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
