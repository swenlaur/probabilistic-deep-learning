{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0106d40",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Important details of GPU programming\n",
    "\n",
    "It is wise to study general principles of GPU programming before delving into deep learning frameworks \n",
    "The following notebook is loosely based on the following sources:\n",
    "\n",
    "* [NVIDIA blog. Inside Pascal](https://developer.nvidia.com/blog/inside-pascal/)\n",
    "* [NVIDIA blog. Unified Memory in CUDA 6](https://developer.nvidia.com/blog/unified-memory-in-cuda-6/)\n",
    "* [NVIDIA blog. CUDA Refresher: The CUDA Programming Model](https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/)\n",
    "* [A. Minnaar. CUDA Grid-Stride Loops: What if you Have More Data Than Threads?](http://alexminnaar.com/2019/08/02/grid-stride-loops.html)\n",
    "* [NVIDIA blog. Using Tensor Cores in CUDA Fortran](https://developer.nvidia.com/blog/using-tensor-cores-in-cuda-fortran/)\n",
    "* [PyTorch blog. Accelerating PyTorch with CUDA Graphs](https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/)\n",
    "* [JAX reference documentation. Asynchronous dispatch](https://jax.readthedocs.io/en/latest/async_dispatch.html)\n",
    "* [TensorFlow resources. XLA: Optimizing Compiler for Machine Learning](https://www.tensorflow.org/xla)\n",
    "* [TensorFlow resources. XLA Architecture](https://www.tensorflow.org/xla/architecture)\n",
    "* [Cloud TPU documentation. System Architecture](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm)\n",
    "* [J. Hui blog. TensorFlow with multiple GPUs](https://jhui.github.io/2017/03/07/TensorFlow-GPU/)\n",
    "* [T. Mayeesha. Introduction to Tensorflow Estimators](https://medium.com/learning-machine-learning/introduction-to-tensorflow-estimators-part-1-39f9eb666bc7)\n",
    "* [T. Verhulsdonck. An Advanced Example of the Tensorflow Estimator Class](https://towardsdatascience.com/an-advanced-example-of-tensorflow-estimators-part-1-3-c9ffba3bff03)\n",
    "* [I. Danish. Learning TensorFlow 2: Use tf.function and Forget About tf.Session](https://irdanish.medium.com/learning-tensorflow-2-use-tf-function-and-forget-about-tf-session-a8117158edd9)\n",
    "* [Tensorflow resources. Estimators](https://www.tensorflow.org/guide/estimator)\n",
    "* [Tensorflow resources. Migrate from Estimator to Keras APIs](https://www.tensorflow.org/guide/migrate/migrating_estimator)\n",
    "* [Tensorflow resources. Keras optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
    "* [A. Rosebrock. Easy Hyperparameter Tuning with Keras Tuner and TensorFlow](https://pyimagesearch.com/2021/06/07/easy-hyperparameter-tuning-with-keras-tuner-and-tensorflow/)\n",
    "* [MJ Bahmani. HyperBand and BOHB: Understanding State of the Art Hyperparameter Optimization Algorithms](https://neptune.ai/blog/hyperband-and-bohb-understanding-state-of-the-art-hyperparameter-optimization-algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a175b8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Roadmap to deep learning frameworks</h1> </center>\n",
    "<center><h2>Important notions</h2></center>\n",
    "<br>\n",
    "<center><h3>Sven Laur</h3></center>\n",
    "<center><h3>swen@ut.ee</h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c5768b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How  GPU-s are used for speeding up the computations? \n",
    "\n",
    "<br>\n",
    "<center><img src=\"./illustrations/4-GPU-CPU-Quad.png\" alt=\"A possible NVIDIA Tesla P100 configuration\" width=\"200\"><center>\n",
    "\n",
    "* CPU-s and GPU-s are connected with high speed memory buses. \n",
    "* Different links have different bandwith and latency parameters.\n",
    "* A data transfer from CPU to GPU can go through OS level buffers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf02caef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How data moves between CPU and GPU?\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"./illustrations/Unified-Memory-MultiGPU-FI.png\" alt=\"Unified memory layout for CUDA 6\" width=\"300\">\n",
    "</center>    \n",
    "\n",
    "* Modern GPU drivers use a unified memory addressing.\n",
    "* Data can be fetched on demand usind standard page fault system. \n",
    "* This allows to use direct memory access and copy on write mechanisms.\n",
    "* Modern GPU hardware has atomic operations and syncronisation mechanisms.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f1a79",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What type of data is sent to GPU?\n",
    "\n",
    "**Kernel:** A function that is executed on GPU.\n",
    "* A kernel uses many threads to evaluate the function.\n",
    "* Threads are grouped into **blocks** for syncronisation. Blocks are grouped into a **grid**. \n",
    "* Threads share the memory but different threads can do different computations. \n",
    "\n",
    "**Grid stride:** A block of memory processed by a single thread.\n",
    "* Usually kernel performs single instruction on multiple data elements.\n",
    "* There might not be enough threads to use a single thread for each data input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9870d53",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What type of data is sent to GPU?\n",
    "\n",
    "**Kernel:** A function that is executed on GPU.\n",
    "* A kernel uses many threads to evaluate the function.\n",
    "* Threads are grouped into **blocks** for syncronisation. Blocks are grouped into a **grid**. \n",
    "* Threads share the memory but different threads can do different computations. \n",
    "\n",
    "**Tensor:** A memory segment that can be treated as a multi-dimensional array.\n",
    "* Tensors are very common in the evaluation of neural networks.\n",
    "* Modern GPU-s have dedicated hardware components (Tensor cores) for them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8166da9f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Three ways to evaluate a neural network\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"./illustrations/cuda-evaluation.png\" alt=\"Benefits of using CUDA graphs\" width=\"500\">\n",
    "</center>\n",
    "    \n",
    "* An evaluation of a neural network consists of several small steps. There are three options for the evaluation.\n",
    "* **Static execution:** You must specify the entire computation before it is executed (**TensorFlow**). \n",
    "* **Dynamic execution:** You can specify operations interactively but have to wait they are completed (**PyTorch**).\n",
    "* **Asynchronous execution:** You can specify operations iteratively but you have to wait only if you explicitly request completion (**JAX**).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4033aac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XLA: A language for Accelerated Linear Algebra\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"./illustrations/XLA_execution.png\" alt=\"XLA execution path\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "* Modern GPU-s use special Tensor Cores that are optimised for vector and matrix operations. \n",
    "* By using XLA compiler the kernel is expressed in terms of special operations.\n",
    "* XLA compiler can be used for static and dynamic execution. \n",
    "* XLA compiler can be applied for a subset of possible Python functions and libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd8bef6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensors in PyTorch\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"./illustrations/pytorch_logo.png\" alt=\"PyTorch logo\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "* A tensor in PyTorch is a multidimensional array in specified device (CPU/GPU).\n",
    "* You can perform operations only tensors with the same scope.   \n",
    "* You cannot control how the tensor is split between different GPU instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e43ed777",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8, 10, 12],\n",
      "        [14, 16, 18]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Place data on specific device \n",
    "x = torch.tensor([[1,2,3],[4,5,6]], device = 'cpu')\n",
    "y = torch.tensor([[7,8,9],[10,11,12]], device = 'cpu') #mps\n",
    "z = x + y\n",
    "\n",
    "# Fetch the tensor to the main memory\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b01b4cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensors in TensorFlow\n",
    "\n",
    "* A tensor in TensorFlow has an execution type:\n",
    "  * **Constant:** the value of the tensor remains the same throughout computations.\n",
    "  * **Variable:** the value of the tensor can be overwrirtten during the execution (**lvalue**).\n",
    "  * **Placeholder:** the value is assigned when the computation is run (**input**). **Not any more!**  \n",
    "  \n",
    "* Tensorflow gives you a fine-grained control on which devices operations are carried out.\n",
    "* Tensors used to be evaluated in the session that executed the computational graph. **Not any more!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "214c1ae4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 8 10 12]\n",
      " [14 16 18]], shape=(2, 3), dtype=int32)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Use standard memory \n",
    "with tf.device('CPU:0'):\n",
    "\n",
    "    x = tf.constant([[1,2,3],[4,5,6]])\n",
    "    y = tf.constant([[7,8,9],[10,11,12]])\n",
    "\n",
    "# Transfer computations to GPU    \n",
    "with tf.device('GPU:0'):    \n",
    "    z = x + y\n",
    "\n",
    "print(z)\n",
    "print(type(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e7a3bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TensorFlow 1.x components\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"./illustrations/tensorflow_components.png\" alt=\"TensorFlow components\" width=\"600\">\n",
    "</center>    \n",
    "\n",
    "* **Estimators:** prepackaged models with standardised training and evaluation loops (analogues of **sklearn** models).\n",
    "* **Model building:** A way to define neural networks from predefined components: layers and activation functions (**keras**).\n",
    "* **Optimisation:** Methods for finding near-best values for parameters and hyperparameters.\n",
    "* **Instrumentation:** a way to observe whaty occurs during training.\n",
    "* **Session:** an old way to perform a single static execution. You can use **tf.function** instead to group operations into a single graph to be evaluated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e496d59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimators vs Keras models\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"./illustrations/tensorflow_interface.png\" alt=\"TensorFlow interface\" width=\"400\">\n",
    "</center>\n",
    "\n",
    "* **Estimator API** the native way to train TensorFlow models\n",
    "* **Keras API** was subsumed by TensorFlow as it was really popular and convenient.\n",
    "* Estimators are now legacy and only predefined models are useful.  **Do not create new custom estimators!**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d65ab26",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining computing kernels with @tf.function \n",
    "\n",
    "* One can chain TensorFlow operations into blocks by defining functions.\n",
    "* These functions will be evaluated step by step and thus there is communication overhead.\n",
    "* Decorator **@tf.function** forces TensorFlow to compile the function into separate kernel.\n",
    "* The compilation might fail if the function body contains **foreign** functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb8c8ac8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "array([[ 8, 10, 12],\n",
       "       [14, 16, 18]], dtype=int32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f():\n",
    "    x = tf.constant([[1,2,3],[4,5,6]])\n",
    "    y = tf.constant([[7,8,9],[10,11,12]])\n",
    "    z = x + y\n",
    "    return z\n",
    "\n",
    "f()\n",
    "\n",
    "@tf.function\n",
    "def compiled_f():\n",
    "    x = tf.constant([[1,2,3],[4,5,6]])\n",
    "    y = tf.constant([[7,8,9],[10,11,12]])\n",
    "    z = x + y\n",
    "    return z\n",
    "\n",
    "# The first execution initiates just-in-time compilation \n",
    "compiled_f()\n",
    "\n",
    "# The seconf execution uses cached code and be much faster\n",
    "compiled_f()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a3146",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instrumentation: What is happening during training?\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"./illustrations/tensorboard.png\" alt=\"Tensorboard\" width=\"700\">\n",
    "</center>\n",
    "    \n",
    "* Keras and Estimator training code contains callback hooks for logging.\n",
    "* Tensorboard is a nice Jupyter extesion that presents this information in a graphical way. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdbbf50",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parameter vs hyperparameter optimisation\n",
    "\n",
    "* Parameter optimisation is not so important as the **data** and **model architecture**.\n",
    "* **Stochastic Gradient Decent** and **Adam** are good enough to get a baseline model.\n",
    "* If not there are many alternatives in [**keras.optimizers**](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) \n",
    "* **Hyperparameters** are nasty! You just cannot apply gradient decent for $\\min_{\\boldsymbol{w}} f(\\boldsymbol{w},\\boldsymbol{h})\\to_{\\boldsymbol{h}}\\min$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b68db97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperparameter optimisation\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"./illustrations/hyperparamater_optimisation.png\" alt=\"Hyperparameter optimisation\" width=\"700\">\n",
    "</center>\n",
    "\n",
    "* Keras Tuner offers several hyperparameter optimizers. The state of the art is **Hyperband**.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
